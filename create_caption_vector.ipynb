{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c98324",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm\n",
    "%pip install numpy\n",
    "%pip install sentencepiece protobuf\n",
    "%pip install python-dotenv==1.2.1\n",
    "%pip install openai==2.16.0\n",
    "%pip install azure-ai-inference==1.0.0b9\n",
    "%pip install sentence-transformers==5.2.2 tiktoken==0.12.0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import numpy  as np\n",
    "import scipy  as sp\n",
    "from dotenv        import load_dotenv\n",
    "from logging       import Logger, getLogger\n",
    "from openai        import OpenAI, AsyncOpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.asyncio  import tqdm_asyncio\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from delta       import configure_spark_with_delta_pip\n",
    "\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from sentence_transformers     import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fe2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder\\\n",
    "            .config(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .config(\"spark.sql.parquet.output.committer.class\", \"org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\")\\\n",
    "            .config(\"spark.mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "            .config(\"spark.sql.adaptive.enabled\", True)\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"auto\")\\\n",
    "            .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"100MB\")\\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\\\n",
    "            .config(\"spark.sql.dynamicPartitionPruning.enabled\", True)\\\n",
    "            .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\\\n",
    "            .config(\"spark.sql.session.timeZone\", \"Asia/Tokyo\")\\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "            .config(\"spark.databricks.delta.write.isolationLevel\", \"SnapshotIsolation\")\\\n",
    "            .config(\"spark.databricks.delta.optimizeWrite.enabled\", True)\\\n",
    "            .config(\"spark.databricks.delta.autoCompact.enabled\", True)\n",
    "            # Delta Lake 用の SQL コミットプロトコルを指定\n",
    "            # Parquet 出力時のコミッタークラスを指定\n",
    "            # Azure Blob Storage (ABFS) 用のコミッターファクトリを指定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # AQE(Adaptive Query Execution)の有効化\n",
    "            # パーティション数を自動で調整するように設定\n",
    "            # シャッフル後の1パーティションあたりの最小サイズを指定\n",
    "            # AQEのパーティション合成の有効化\n",
    "            # 動的パーティションプルーニングの有効化\n",
    "            # 小さいテーブルのブロードキャスト結合への自動変換をするための閾値調整\n",
    "            # SparkSessionのタイムゾーンを日本標準時刻に設定\n",
    "            # Delta Lake固有のSQL構文や解析機能を拡張モジュールとして有効化\n",
    "            # SparkカタログをDeltaLakeカタログへ変更\n",
    "            # Delta Lake書き込み時のアイソレーションレベルを「スナップショット分離」に設定\n",
    "            # 書き込み時にデータシャッフルを行い、大きなファイルを生成する機能の有効化\n",
    "            # 書き込み後に小さなファイルを自動で統合する機能の有効化\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29fbb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env ファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# 環境変数の取得\n",
    "AI_FOUNDRY_ENDPOINT = os.environ.get(\"AI_FOUNDRY_ENDPOINT\")\n",
    "AI_FOUNDRY_API_KEY  = os.environ.get(\"AI_FOUNDRY_API_KEY\")\n",
    "AI_FOUNDRY_MODEL    = os.environ.get(\"AI_FOUNDRY_MODEL\")\n",
    "MAX_TOKENS          = os.environ.get(\"MAX_TOKENS\")\n",
    "TEMPERATURE         = os.environ.get(\"TEMPERATURE\")\n",
    "TOP_P               = os.environ.get(\"TOP_P\")\n",
    "\n",
    "# メモ：\n",
    "# ウィジット経由でのパラメータの取得方法では、無条件にパラメータの型がStringに変換されてしまう\n",
    "# そのため、受け取ったパラメータを適切に型変換する必要がある\n",
    "MAX_TOKENS  = int(MAX_TOKENS)\n",
    "TEMPERATURE = float(TEMPERATURE)\n",
    "TOP_P       = float(TOP_P)\n",
    "\n",
    "\n",
    "# 簡易デバッグ用\n",
    "print(f'AI_FOUNDRY_ENDPOINT: {AI_FOUNDRY_ENDPOINT}')\n",
    "print(f'AI_FOUNDRY_API_KEY:  {AI_FOUNDRY_API_KEY}')\n",
    "print(f'AI_FOUNDRY_MODEL:    {AI_FOUNDRY_MODEL}')\n",
    "print(f'MAX_TOKENS:          {MAX_TOKENS}')\n",
    "print(f'TEMPERATURE:         {TEMPERATURE}')\n",
    "print(f'TOP_P:               {TOP_P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモ：\n",
    "# cohort.npzは以下のようなデータ構成になっている\n",
    "# cohort.npz\n",
    "#     |-- data              : 計算済みコホート係数行列\n",
    "#     |-- indices           : データの位置指定子(列)\n",
    "#     |-- indptr            : 行ごとのスライスインデックス\n",
    "#     |-- shape             : コホート係数行列の形(ADIDのリスト × コホートキャプションのリスト)\n",
    "#     |-- adid_list         : ADIDのリスト(列)\n",
    "#     |-- business_codelist : コホートキャプションIDのリスト(行)\n",
    "TARGET      = \"/Volumes/stgadintedmpadintedi/featurestore/behaviorvector/cohort.npz\"\n",
    "npz         = np.load(TARGET, allow_pickle=True)\n",
    "np_codelist = npz[\"business_codelist\"]\n",
    "\n",
    "\n",
    "# CODELISTに対するキャプションリストを取得\n",
    "NAVIT_BUSINESS_TABLE = \"adinte_datainfrastructure.list.navit_business\"\n",
    "sdf_navit_business   = spark.read.table(NAVIT_BUSINESS_TABLE)\\\n",
    "\t\t\t\t\t\t\t.select(['BUSINESS_CODE', 'BUSINESS_NAME_S'])\\\n",
    "                            .join(spark.createDataFrame([(elem,) for elem in np_codelist.tolist()], ['BUSINESS_CODE']), on='BUSINESS_CODE', how='inner')\n",
    "\n",
    "sdf_navit_business.toPandas().to_csv('cohort_caption_list.csv', index=False, header=True)\n",
    "pdf_navit_business   = pd.read_csv('cohort_caption_list.csv', header=0)\n",
    "dict_code2name       = pdf_navit_business.set_index('BUSINESS_CODE')['BUSINESS_NAME_S'].to_dict()\n",
    "dict_code2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dc6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmClient  = AsyncOpenAI(base_url=AI_FOUNDRY_ENDPOINT, api_key=AI_FOUNDRY_API_KEY)\n",
    "system_msg = SystemMessage(content=(\n",
    "\t\t\t\t\"あなたは、特定の場所・施設名から、そこに集まる人々の輪郭を鮮明に描き出す高度な空間データアナリストです。\\n\"\n",
    "                \"それぞれの場所に対し、その空間の特性、利用者の属性、行動パターンを深掘りし、以下のJSON形式で分析結果を出力してください。\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"【出力形式（厳守）】\\n\"\n",
    "\t\t\t\t\"回答は必ず以下のJSON形式のみを出力してください。\\n\"\n",
    "\t\t\t\t\"各場所・施設名をキーとし、150文字以内の短文としてまとめてください。\\n\"\n",
    "\t\t\t\t\"Markdown記法（```json 等）は含めず、生のJSONテキストのみを返してください。\\n\\n\"\n",
    "                \"\"\"\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"入力された場所・施設名1\": [\"具体的なペルソナと行動1（属性＋場所＋行為）\", \"具体的なペルソナと行動2（属性＋場所＋行為）\", ...],\n",
    "\t\t\t\t\t\"入力された場所・施設名2\": [\"具体的なペルソナと行動3（属性＋場所＋行為）\", ...],\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t\"\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【抽出・生成のルール】\\n\"\n",
    "\t\t\t\t\"1. 語彙の解像度（1文120文字程度を推奨）\\n\"\n",
    "                \"- 単語の羅列ではなく、情景が浮かぶ「属性＋具体的な状況＋行動」のセットで記述してください。\\n\"\n",
    "\t\t\t\t\"- NG例: 「公園」「運動」「サラリーマン」\\n\"\n",
    "\t\t\t\t\"- OK例: 「週末の親水公園で大型犬を遊ばせる愛犬家」「深夜の24時間ジムで大会に向けて追い込むトレーニー」「早朝の駅ビルでPC作業をするノマドワーカー」\\n\"\n",
    "\t\t\t\t\"2. 出力ボリューム\\n\"\n",
    "\t\t\t\t\"- 入力された場所・施設名、7つずつ抽出してください。\\n\\n\"\n",
    "\t\t\t))\n",
    "\n",
    "async def fetch_analysis(semaphore:asyncio.Semaphore, user_msg:str):\n",
    "\tfor idx in range(10):\n",
    "\t\tidx_temperature = TEMPERATURE * (0.9 ** idx)\n",
    "\t\ttry:\n",
    "\t\t\tasync with semaphore:\n",
    "\t\t\t\tresponse     = await llmClient.chat.completions.create(\n",
    "\t\t\t\t\t\t\t\t\t\tmessages=[system_msg, user_msg],\n",
    "\t\t\t\t\t\t\t\t\t\ttools=None,\n",
    "\t\t\t\t\t\t\t\t\t\ttool_choice=None,\n",
    "\t\t\t\t\t\t\t\t\t\tmax_tokens=MAX_TOKENS,\n",
    "\t\t\t\t\t\t\t\t\t\ttemperature=idx_temperature,\n",
    "\t\t\t\t\t\t\t\t\t\ttop_p=TOP_P,\n",
    "\t\t\t\t\t\t\t\t\t\tmodel=AI_FOUNDRY_MODEL\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\traw_content  = response.choices[0].message.content\n",
    "\n",
    "\t\t\tcontent_dict = json.loads(raw_content)\n",
    "\t\t\treturn content_dict\n",
    "\t\t\t\n",
    "\t\texcept json.JSONDecodeError as e:\n",
    "\t\t\tprint(f\"[Attempt {idx+1}] Decode Error:: {e}\")\n",
    "\t\t\tprint(raw_content)\n",
    "\t\t\tprint()\n",
    "\n",
    "\t\t\t# 一旦、サーバー負荷の軽減しつつリトライ\n",
    "\t\t\tawait asyncio.sleep(1)\n",
    "\t\t\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"[Attempt {idx+1}] API Error: {e}\")\n",
    "\n",
    "\t\t\t# 一旦、サーバー負荷の軽減しつつリトライ\n",
    "\t\t\tawait asyncio.sleep(3)\n",
    "\t\n",
    "\tprint(f\"Failed all 10 attempts for this batch.\")\n",
    "\treturn {}\n",
    "\n",
    "async def main():\n",
    "    semaphore = asyncio.Semaphore(10)\n",
    "    tasks     = []\n",
    "    for idx in range(0, len(data_list), 2):\n",
    "        pair_place = data_list[idx : idx+2]\n",
    "        user_msg   = UserMessage(content=json.dumps(pair_place))\n",
    "        tasks.append(fetch_analysis(semaphore, user_msg))\n",
    "    \n",
    "    results       = await tqdm_asyncio.gather(*tasks)\n",
    "    analysis_data = {}\n",
    "    for result in results:\n",
    "        analysis_data |= result\n",
    "    \n",
    "    return analysis_data\n",
    "\n",
    "data_list     = [dict_code2name[int(elem)] for elem in np_codelist.tolist()]\n",
    "analysis_data = await main()\n",
    "analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c07ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model            = SentenceTransformer('pkshatech/GLuCoSE-base-ja')\n",
    "count_list       = [len(analysis_data[key]) for key in analysis_data]\n",
    "list_location    = [scene                   for key in analysis_data for scene in analysis_data[key]]\n",
    "np_matrix        = model.encode(list_location)                     # (スポット数N × シーン数M) × 768\n",
    "\n",
    "\n",
    "indices      = np.cumsum(count_list)[:-1]\n",
    "final_matrix = np.array([chunk.mean(axis=0) for chunk in np.split(np_matrix, indices)])\n",
    "final_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8f577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
