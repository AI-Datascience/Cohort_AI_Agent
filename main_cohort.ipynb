{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install sentencepiece protobuf\n",
    "%pip install polars==1.38.1\n",
    "%pip install python-dotenv==1.2.1\n",
    "%pip install openai==2.16.0\n",
    "%pip install readability-lxml==0.8.4.1 w3lib==2.4.0 httpx==0.28.1 beautifulsoup4==4.14.3 azure-ai-inference==1.0.0b9\n",
    "%pip install sentence-transformers==5.2.2 tiktoken==0.12.0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6af442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import numpy  as np\n",
    "import scipy  as sp\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv   import load_dotenv\n",
    "from logging  import Logger, getLogger\n",
    "from openai   import OpenAI\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import types, Window\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from sentence_transformers     import SentenceTransformer\n",
    "\n",
    "from url_scraper import fetch_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f88cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder\\\n",
    "            .config(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .config(\"spark.sql.parquet.output.committer.class\", \"org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\")\\\n",
    "            .config(\"spark.mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "            .config(\"spark.sql.adaptive.enabled\", True)\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"auto\")\\\n",
    "            .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"100MB\")\\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\\\n",
    "            .config(\"spark.sql.dynamicPartitionPruning.enabled\", True)\\\n",
    "            .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\\\n",
    "            .config(\"spark.sql.session.timeZone\", \"Asia/Tokyo\")\\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "            .config(\"spark.databricks.delta.write.isolationLevel\", \"SnapshotIsolation\")\\\n",
    "            .config(\"spark.databricks.delta.optimizeWrite.enabled\", True)\\\n",
    "            .config(\"spark.databricks.delta.autoCompact.enabled\", True)\n",
    "            # Delta Lake 用の SQL コミットプロトコルを指定\n",
    "            # Parquet 出力時のコミッタークラスを指定\n",
    "            # Azure Blob Storage (ABFS) 用のコミッターファクトリを指定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # AQE(Adaptive Query Execution)の有効化\n",
    "            # パーティション数を自動で調整するように設定\n",
    "            # シャッフル後の1パーティションあたりの最小サイズを指定\n",
    "            # AQEのパーティション合成の有効化\n",
    "            # 動的パーティションプルーニングの有効化\n",
    "            # 小さいテーブルのブロードキャスト結合への自動変換をするための閾値調整\n",
    "            # SparkSessionのタイムゾーンを日本標準時刻に設定\n",
    "            # Delta Lake固有のSQL構文や解析機能を拡張モジュールとして有効化\n",
    "            # SparkカタログをDeltaLakeカタログへ変更\n",
    "            # Delta Lake書き込み時のアイソレーションレベルを「スナップショット分離」に設定\n",
    "            # 書き込み時にデータシャッフルを行い、大きなファイルを生成する機能の有効化\n",
    "            # 書き込み後に小さなファイルを自動で統合する機能の有効化\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b343d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env ファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# 環境変数の取得\n",
    "AI_FOUNDRY_ENDPOINT = os.environ.get(\"AI_FOUNDRY_ENDPOINT\")\n",
    "AI_FOUNDRY_API_KEY  = os.environ.get(\"AI_FOUNDRY_API_KEY\")\n",
    "AI_FOUNDRY_MODEL    = os.environ.get(\"AI_FOUNDRY_MODEL\")\n",
    "MAX_TOKENS          = os.environ.get(\"MAX_TOKENS\")\n",
    "TEMPERATURE         = os.environ.get(\"TEMPERATURE\")\n",
    "TOP_P               = os.environ.get(\"TOP_P\")\n",
    "\n",
    "# メモ：\n",
    "# ウィジット経由でのパラメータの取得方法では、無条件にパラメータの型がStringに変換されてしまう\n",
    "# そのため、受け取ったパラメータを適切に型変換する必要がある\n",
    "MAX_TOKENS  = int(MAX_TOKENS)\n",
    "TEMPERATURE = float(TEMPERATURE)\n",
    "TOP_P       = float(TOP_P)\n",
    "\n",
    "\n",
    "# 簡易デバッグ用\n",
    "print(f'AI_FOUNDRY_ENDPOINT: {AI_FOUNDRY_ENDPOINT}')\n",
    "print(f'AI_FOUNDRY_API_KEY:  {AI_FOUNDRY_API_KEY}')\n",
    "print(f'AI_FOUNDRY_MODEL:    {AI_FOUNDRY_MODEL}')\n",
    "print(f'MAX_TOKENS:          {MAX_TOKENS}')\n",
    "print(f'TEMPERATURE:         {TEMPERATURE}')\n",
    "print(f'TOP_P:               {TOP_P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmClient = OpenAI(base_url=AI_FOUNDRY_ENDPOINT, api_key=AI_FOUNDRY_API_KEY)\n",
    "logger    = getLogger(__name__)\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "# 例とするランディングページ\n",
    "LP_URL   = \"https://lp.br-lb.com/\"\n",
    "\n",
    "res_dict = await fetch_web(logger, semaphore, LP_URL)\n",
    "print(res_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "540046c8-0091-4d3a-8617-2055c9eddfea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 前処理データ部\n",
    "\n",
    "INPUT:\n",
    "- LLMによる商品LPの分析結果\n",
    "- ADIDの行動特徴テーブル\n",
    "\n",
    "OUTPUT:\n",
    "- [1, コホートキャプション数]なLPのスポット係数ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da514f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages  = []\n",
    "messages.append(SystemMessage(content=(\n",
    "\t\t\t\t\"あなたは商品LPの分析を行うマーケティングの専門家です。\\n\"\n",
    "\t\t\t\t\"提供された商品情報を分析し、その商品が「最高に輝く具体的なシーン（適合）」と「全く役に立たない、あるいはミスマッチなシーン（不適合）」を洗い出してください。\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【重要な指示】\\n\"\n",
    "                \"出力するキーワードは、ベクトル検索のクエリとして使用されます。\\n\"\n",
    "\t\t\t\t\"そのため、単一の一般名詞（例：「公園」「オフィス」）は **禁止** です。\\n\"\n",
    "                \"必ず **「場所＋状況」** または **「属性＋場所」** の複合キーワード（Micro-Context）を選定してください。\\n\\n\"\n",
    "    \t\t\t\"抽出する単語は、単なる一般名詞ではなく、「誰が・どこで・何をしているか」がありありと想像できるような、具体的かつ解像度の高いキーワードを選定してください。\\n\"\n",
    "    \t\t\t\"特に「場所」に関しては、大分類（例：公園）ではなく、詳細な施設タイプ（例：ドッグラン、親水広場）や、利用目的が明確なスポット名を優先してください。\\n\\n\"\n",
    "                \"LPに直接記載がなくても、商品の特性から論理的に推測されるシーンは積極的に広げて記述してください。\\n\\n\"\n",
    "                \"・NG例: 「ジム」「キャンプ」「サラリーマン」\\n\"\n",
    "                \"・OK例: 「深夜の24時間ジム」「雨上がりのオートキャンプ場」「満員電車の通勤客」「コンセントのあるカフェ席」\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"【出力形式（厳守）】\\n\"\n",
    "\t\t\t\t\"回答は必ず以下のJSON形式のみを出力してください。\\n\"\n",
    "\t\t\t\t\"各単語をキーとし、その関連度の強さ（重み）を 0.0〜1.0 の数値で値として設定してください。\\n\"\n",
    "\t\t\t\t\"Markdown記法（```json 等）は含めず、生のJSONテキストのみを返してください。\\n\\n\"\n",
    "                \"・Positive/Negative 共に、確信度の高い上位5〜10個程度を抽出してください。\\n\\n\"\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"positive\": {\"複合キーワードA\": 0.89, \"キーワードB\": 0.70, ...},\n",
    "\t\t\t\t\t\"negative\": {\"複合キーワードC\": 0.91, ...},\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t\"\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【分析の視点】\\n\"\n",
    "\t\t\t\t\"--- positive（適合）：商品が必須となる、または魅力を最大化する文脈 ---\\n\"\n",
    "\t\t\t\t\"1. 具体的な施設・スポット（Places）：\\n\"\n",
    "\t\t\t\t\"   - 抽象的な「店」「屋外」はNG。\\n\"\n",
    "\t\t\t\t\"   - 「24時間ジム」「オートキャンプ場」「コワーキングスペース」など、行動が特定できる施設名。\\n\"\n",
    "\t\t\t\t\"2. 利用シーン・瞬間（Scenes）：\\n\"\n",
    "\t\t\t\t\"   - 「通勤ラッシュ」「運動後のシャワー」「子供の寝かしつけ」など、具体的なタイムラインや状況。\\n\"\n",
    "\t\t\t\t\"3. ターゲットの属性・状態（Traits）：\\n\"\n",
    "\t\t\t\t\"   - 「健康志向」のような広い言葉より、「糖質制限中」「リモートワーク疲れ」など具体的な状態。\\n\\n\"\n",
    "                \"4. 物理的適合 (Physical Fit):\\n\"\n",
    "                \"   - 商品のサイズ、電源、耐久性が、その場所の設備・環境と完璧に噛み合うか。\\n\"\n",
    "                \"   - マグネットでくっつく防水Bluetoothスピーカー  →  「ユニットバスの壁面」「雨の日のキャンプのタープ下」\"\n",
    "\t\t\t\t\"5. 心理的・行動的適合 (Contextual Fit):\\n\"\n",
    "                \"   - その場所にいる人の「特定の悩み」を解決するか。\\n\"\n",
    "\t\t\t\t\"   - 周囲の音を消すデジタル耳栓  →  「いびきが気になるカプセルホテル」「瞑想に集中したいヨガスタジオの隅」\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"--- negative（不適合）：商品の機能が死ぬ、または邪魔になる文脈 ---\\n\"\n",
    "\t\t\t\t\"1. 阻害要因となる場所（Places）：\\n\"\n",
    "\t\t\t\t\"   - 商品のスペック（大きさ、音、電源有無など）的に使えない場所（例：図書館、満員電車）。\\n\"\n",
    "\t\t\t\t\"2. 無意味なシーン（Scenes）：\\n\"\n",
    "\t\t\t\t\"   - その商品をあえて使う必要がない状況。\\n\\n\"\n",
    "                \"3. 環境不適合 (Environmental Mismatch):\\n\"\n",
    "                \"   - 商品を使うには狭すぎる、暗すぎる、うるさすぎる、静かすぎる場所。\\n\"\n",
    "                \"4. マナー・ルール違反 (Social Mismatch):\\n\"\n",
    "                \"   - その場所でその商品を使うことが「白い目」で見られる、あるいは禁止されている。\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【重み（スコア）の基準】\\n\"\n",
    "\t\t\t\t\"・1.0に近いほど：その傾向が非常に強い、確信度が高い\\n\"\n",
    "\t\t\t\t\"・0.0に近いほど：関連性が薄い\\n\"\n",
    "                \"・1.0: 完全にその商品の独壇場である（または絶対に使用不可である）。\\n\"\n",
    "                \"・0.8: 非常に相性が良い（または強い懸念がある）。\\n\"\n",
    "                \"・0.5: 条件による（今回は出力対象外）。\\n\"\n",
    "\t\t\t\t\"・positiveの場合：適合度の高さ\\n\"\n",
    "\t\t\t\t\"・negativeの場合：不適合度の高さ（明確に避けるべき度合い）\"\n",
    "\t\t\t)))\n",
    "messages.append(UserMessage(content=json.dumps(res_dict, indent=4, ensure_ascii=False)))\n",
    "response      = llmClient.chat.completions.create(\n",
    "\t\t\t\t\tmessages=messages,\n",
    "\t\t\t\t\ttools=None,\n",
    "\t\t\t\t\ttool_choice=None,\n",
    "\t\t\t\t\tmax_tokens=MAX_TOKENS,\n",
    "\t\t\t\t\ttemperature=TEMPERATURE,\n",
    "\t\t\t\t\ttop_p=TOP_P,\n",
    "\t\t\t\t\tmodel=AI_FOUNDRY_MODEL\n",
    "\t\t\t\t)\n",
    "\n",
    "result_text   = response.choices[0].message.content\n",
    "analysis_data = json.loads(result_text)\n",
    "analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモ：\n",
    "# cohort.npzは以下のようなデータ構成になっている\n",
    "# cohort.npz\n",
    "#     |-- data              : 計算済みコホート係数行列\n",
    "#     |-- indices           : データの位置指定子(列)\n",
    "#     |-- indptr            : 行ごとのスライスインデックス\n",
    "#     |-- shape             : コホート係数行列の形(ADIDのリスト × コホートキャプションのリスト)\n",
    "#     |-- adid_list         : ADIDのリスト(列)\n",
    "#     |-- business_codelist : コホートキャプションIDのリスト(行)\n",
    "TARGET      = \"/Volumes/stgadintedmpadintedi/featurestore/behaviorvector/cohort.npz\"\n",
    "npz         = np.load(TARGET, allow_pickle=True)\n",
    "np_cohort   = sp.sparse.csr_matrix((npz[\"data\"], npz[\"indices\"], npz[\"indptr\"]), shape=tuple(npz[\"shape\"]))\n",
    "np_adidlist = npz[\"adid_list\"]\n",
    "np_codelist = npz[\"business_codelist\"]\n",
    "\n",
    "\n",
    "# CODELISTに対応するキャプションの文脈行列を取得\n",
    "CAPTION_CONTEXT_MATRIX = \"cohort_caption_matrix.npz\"\n",
    "npz                    = np.load(CAPTION_CONTEXT_MATRIX, allow_pickle=True)\n",
    "spots_matrix           = npz[\"data\"]\n",
    "relational_spots       = npz[\"business_placelist\"]\n",
    "dict_code2name         = npz[\"dict_code2name\"].item()\n",
    "\n",
    "# 簡易チェック\n",
    "assert all(code in dict_code2name for code in np_codelist), \"There are unregistered keys.\"\n",
    "assert np.array_equal(np.array([dict_code2name[code] for code in np_codelist]), relational_spots), \"Location list is inconsistent.\"\n",
    "\n",
    "print(np_cohort.shape)\n",
    "print(spots_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_positive   = list(analysis_data['positive'].items())\n",
    "items_negative   = list(analysis_data['negative'].items())\n",
    "lp_keywords      = [key for key, val in items_positive] + [ key for key, val in items_negative]\n",
    "lp_weights       = [val for key, val in items_positive] + [-val for key, val in items_negative]\n",
    "\n",
    "model            = SentenceTransformer('cl-nagoya/ruri-v3-130m')\n",
    "lp_vector        = np.array(lp_weights).reshape(1, -1)               # 1 × キーワード数M\n",
    "lp_matrix        = model.encode(lp_keywords)                         # キーワード数M × 512\n",
    "lp_coefficient   = lp_vector @ lp_matrix @ spots_matrix.T            # LPのスポット係数\n",
    "lp_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c2d7641-64ab-4ebb-8ddd-6bc15434175f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ADID毎のスコア・理由を算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RECORDS      = 10000\n",
    "MAX_VALUE        = np.max( lp_coefficient)\n",
    "MIN_VALUE        = np.min( lp_coefficient)\n",
    "CORRECT_MEAN     = np.mean(lp_coefficient)\n",
    "CORRECT_STDDEV   = np.std( lp_coefficient)\n",
    "lp_coefficient   = 2 * (lp_coefficient - MIN_VALUE) / (MAX_VALUE - MIN_VALUE) - 1\n",
    "# lp_coefficient   = (lp_coefficient - CORRECT_MEAN) / CORRECT_STDDEV\n",
    "\n",
    "# ADID毎のスコアを算出\n",
    "np_scored        = (np_cohort @ lp_coefficient.T).flatten()\n",
    "indices          = np.argsort(np_scored)[::-1][:MAX_RECORDS]\n",
    "sorted_adidlist  = np_adidlist[indices]\n",
    "sorted_spots     = relational_spots\n",
    "sorted_targets   = np_cohort[indices, :]\n",
    "sorted_scored    = np_scored[indices]\n",
    "\n",
    "# 閾値以上のスポットを理由とする\n",
    "REASON_THRESHOLD = 0.5\n",
    "# REASON_THRESHOLD = 1.28\n",
    "np_threshold     = lp_coefficient > REASON_THRESHOLD\n",
    "np_reasons       = sorted_targets.multiply(np_threshold)\n",
    "np_reasons.eliminate_zeros()\n",
    "pldf_reasons     = pl.DataFrame({\n",
    "    \t\t\t\t\t\t'ADID'           : sorted_adidlist[np_reasons.row], \n",
    "                            'cohort_caption' : sorted_spots[np_reasons.col], \n",
    "                            'score'          : sorted_scored[np_reasons.row], \n",
    "                            'value'          : np_reasons.data\n",
    "                        })\\\n",
    "\t\t\t\t\t\t.filter(pl.col('value') > 0)\\\n",
    "                        .group_by(pl.col('ADID'), maintain_order=True)\\\n",
    "                        .agg(\n",
    "                            pl.col('score').first(),\n",
    "                            pl.col('cohort_caption').str.join(', ').alias('reasons')\n",
    "                        )\\\n",
    "                        .select(['ADID', 'score', 'reasons'])\n",
    "\n",
    "\n",
    "print(pldf_reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_spots = sorted_spots[np_threshold.flatten()]\n",
    "messages        = []\n",
    "messages.append(SystemMessage(content=(\n",
    "\t\t\t\t\"あなたは、複数の場所・施設名から、それらを行動範囲とする人物の「ライフスタイル」や「価値観」を逆引きで特定するプロファイリングの専門家です。\\n\"\n",
    "\t\t\t\t\"**入力された全ての場所・施設を利用する可能性が高い人物像**を分析し、その共通項（Intersection）から浮かび上がる具体的なペルソナを3人描き出してください。\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"【出力形式（厳守）】\\n\"\n",
    "\t\t\t\t\"回答は必ず以下のJSON形式のみを出力してください。\\n\"\n",
    "\t\t\t\t\"キーには「persona_1」「persona_2」...を使用し、値に分析結果のテキストを入れてください。\\n\"\n",
    "\t\t\t\t\"Markdown記法（```json 等）は含めず、生のJSONテキストのみを返してください。\\n\\n\"\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"persona_1\": \"分析された具体的なペルソナ像A（属性＋状況＋行動）\",\n",
    "        \t\t\t\"persona_2\": \"分析された具体的なペルソナ像B（属性＋状況＋行動）\",\n",
    "        \t\t\t\"persona_2\": \"分析された具体的なペルソナ像B（属性＋状況＋行動）\"\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t\"\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【分析プロセス（重要）】\\n\"\n",
    "\t\t\t\t\"1. 点をつなぐ: 入力された施設A、施設B、施設C...の全てに親和性があるのはどのような人物か？\\n\"\n",
    "\t\t\t\t\"2. 背景を読む: なぜその人はそれらの場所を選ぶのか？（共通するニーズ、美意識、所得層、時間の使い方）\\n\"\n",
    "\t\t\t\t\"3. 解像度を上げる: 下記のルールに従い、具体的な情景として出力する。\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"【抽出・生成のルール】\\n\"\n",
    "\t\t\t\t\"1. 語彙の解像度（Micro-Context）\\n\"\n",
    "\t\t\t\t\"   - 単なる属性名ではなく、「属性＋具体的な状況＋心理/行動」のセットで記述してください。\\n\"\n",
    "\t\t\t\t\"   - 抽象的な表現（例：富裕層、サラリーマン）は **禁止** です。\\n\"\n",
    "\t\t\t\t\"   - NG: 「週末にジムとカフェに行く会社員」\\n\"\n",
    "\t\t\t\t\"   - OK: 「週末の午前中にパーソナルジムで汗を流した後、ラウンジでプロテインを飲みながら投資信託のチェックをするアッパーマス層」\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"2. 多角的な視点の統合\\n\"\n",
    "\t\t\t\t\"   - 複数の施設に共通する「空気感」や「質」へのこだわりを盛り込む。\\n\"\n",
    "\t\t\t\t\"   - 【時間軸】: 彼らはいつ現れるか（早朝のルーティン、深夜の逃避など）\\n\"\n",
    "\t\t\t\t\"   - 【五感・嗜好】: 彼らが好む環境（静寂、活気、オーガニック、ラグジュアリーなど）\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"3. 文体\\n\"\n",
    "\t\t\t\t\"   - 「〜です」「〜ます」は不要。体言止めや現在進行形で、その人物の生活を切り取るように記述する。\\n\"\n",
    "\t\t\t\t\"   - 1文あたり120文字程度。\"\n",
    "\t\t\t)))\n",
    "messages.append(UserMessage(content=json.dumps(res_dict, indent=4, ensure_ascii=False)))\n",
    "response     = llmClient.chat.completions.create(\n",
    "\t\t\t\t\tmessages=messages,\n",
    "\t\t\t\t\ttools=None,\n",
    "\t\t\t\t\ttool_choice=None,\n",
    "\t\t\t\t\tmax_tokens=MAX_TOKENS,\n",
    "\t\t\t\t\ttemperature=TEMPERATURE,\n",
    "\t\t\t\t\ttop_p=TOP_P,\n",
    "\t\t\t\t\tmodel=AI_FOUNDRY_MODEL\n",
    "\t\t\t\t)\n",
    "\n",
    "result_text  = response.choices[0].message.content\n",
    "persona_data = json.loads(result_text)\n",
    "persona_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_spots = sorted_spots[np_threshold.flatten()]\n",
    "messages        = []\n",
    "messages.append(SystemMessage(content=(\n",
    "\t\t\t\t\"あなたは、消費者心理を深く洞察し、売れる文脈を設計する「マーケティング・ストラテジスト」です。\\n\"\n",
    "    \t\t\t\"提供された「商品LP情報」と「ターゲットペルソナ」を分析し、そのターゲットの心を動かす**具体的な利用想起シナリオ（訴求アングル）**を2パターン開発してください。\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"【出力形式（厳守）】\\n\"\n",
    "\t\t\t\t\"回答は必ず以下のJSON形式のみを出力してください。\\n\"\n",
    "\t\t\t\t\"Markdown記法（```json 等）は含めず、生のJSONテキストのみを返してください。\\n\\n\"\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"scenario_1\": {\n",
    "\t\t\t\t\t\t\"title\":          \"シナリオのタイトル（コンセプト名）\",\n",
    "\t\t\t\t\t\t\"target_insight\": \"ペルソナが抱えている隠れた本音・悩み（インサイト）\",\n",
    "\t\t\t\t\t\t\"product_match\":  \"LP内のどの要素が解決策になるか（具体的な機能・特徴）\",\n",
    "\t\t\t\t\t\t\"usage_scene\":    \"商品を利用している、または商品を欲しくなる具体的な情景描写（Micro-Context）\",\n",
    "\t\t\t\t\t\t\"catchphrase\":    \"そのペルソナに刺さるキャッチコピー\"\n",
    "\t\t\t\t\t},\n",
    "        \t\t\t\"scenario_2\": { ... }\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t\"\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【分析プロセス（重要）】\\n\"\n",
    "\t\t\t\t\"1. ペルソナの生活を憑依させる: その人は普段どんな場所にいて、どんな悩みを抱え、何を求めているか？\\n\"\n",
    "    \t\t\t\"2. 接点を見つける:          商品のどの機能（スペック）が、そのペルソナのどの課題（ペイン）を解決するか？\\n\"\n",
    "    \t\t\t\"3. シチュエーションを描く:    その商品が最も魅力的に映る「具体的な瞬間」を切り取る。\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"【品質基準】\\n\"\n",
    "                \"- 表面的なマッチング（例：太っているからジムへ）は禁止。\\n\"\n",
    "\t\t\t\t\"- 「なぜそのペルソナなのか」が論理的に紐付いていること。\\n\"\n",
    "\t\t\t\t\"- 情景描写は五感に訴えるレベル具体的に（例：「残業終わりの重い足取りで...」）。\\n\"\n",
    "\t\t\t\t\"- 商品情報は必ず入力されたLPデータに基づいていること（捏造しない）。\"\n",
    "\t\t\t)))\n",
    "user_input_text = f\"\"\"\n",
    "【分析対象データ】\n",
    "\n",
    "1. 商品LP情報 (Product):\n",
    "{json.dumps(res_dict,     indent=4, ensure_ascii=False)}\n",
    "\n",
    "2. ターゲットペルソナ (Target Persona):\n",
    "{json.dumps(persona_data, indent=4, ensure_ascii=False)}\n",
    "\n",
    "この商品とペルソナの「運命的な出会い」を演出するシナリオを作成してください。\n",
    "\"\"\"\n",
    "messages.append(UserMessage(content=user_input_text))\n",
    "response     = llmClient.chat.completions.create(\n",
    "\t\t\t\t\tmessages=messages,\n",
    "\t\t\t\t\ttools=None,\n",
    "\t\t\t\t\ttool_choice=None,\n",
    "\t\t\t\t\tmax_tokens=MAX_TOKENS,\n",
    "\t\t\t\t\ttemperature=TEMPERATURE,\n",
    "\t\t\t\t\ttop_p=TOP_P,\n",
    "\t\t\t\t\tmodel=AI_FOUNDRY_MODEL\n",
    "\t\t\t\t)\n",
    "\n",
    "result_text  = response.choices[0].message.content\n",
    "appeal_scenario_data = json.loads(result_text)\n",
    "appeal_scenario_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_cohort",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
