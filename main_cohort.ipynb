{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==2.4.2\n",
    "%pip install python-dotenv==1.2.1\n",
    "%pip install openai==2.16.0\n",
    "%pip install readability-lxml==0.8.4.1 w3lib==2.4.0 httpx==0.28.1 beautifulsoup4==4.14.3 azure-ai-inference==1.0.0b9\n",
    "%pip install sentencepiece==0.2.1 protobuf==6.33.5\n",
    "%pip install sentence-transformers==5.2.2 tiktoken==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6af442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import numpy  as np\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv   import load_dotenv\n",
    "from logging  import Logger, getLogger\n",
    "from openai   import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import types, Window\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from sentence_transformers     import SentenceTransformer\n",
    "\n",
    "from url_scraper import fetch_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f88cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder\\\n",
    "            .config(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .config(\"spark.sql.parquet.output.committer.class\", \"org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\")\\\n",
    "            .config(\"spark.mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "            .config(\"spark.sql.adaptive.enabled\", True)\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"auto\")\\\n",
    "            .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"100MB\")\\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\\\n",
    "            .config(\"spark.sql.dynamicPartitionPruning.enabled\", True)\\\n",
    "            .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\\\n",
    "            .config(\"spark.sql.session.timeZone\", \"Asia/Tokyo\")\\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "            .config(\"spark.databricks.delta.write.isolationLevel\", \"SnapshotIsolation\")\\\n",
    "            .config(\"spark.databricks.delta.optimizeWrite.enabled\", True)\\\n",
    "            .config(\"spark.databricks.delta.autoCompact.enabled\", True)\n",
    "            # Delta Lake 用の SQL コミットプロトコルを指定\n",
    "            # Parquet 出力時のコミッタークラスを指定\n",
    "            # Azure Blob Storage (ABFS) 用のコミッターファクトリを指定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # AQE(Adaptive Query Execution)の有効化\n",
    "            # パーティション数を自動で調整するように設定\n",
    "            # シャッフル後の1パーティションあたりの最小サイズを指定\n",
    "            # AQEのパーティション合成の有効化\n",
    "            # 動的パーティションプルーニングの有効化\n",
    "            # 小さいテーブルのブロードキャスト結合への自動変換をするための閾値調整\n",
    "            # SparkSessionのタイムゾーンを日本標準時刻に設定\n",
    "            # Delta Lake固有のSQL構文や解析機能を拡張モジュールとして有効化\n",
    "            # SparkカタログをDeltaLakeカタログへ変更\n",
    "            # Delta Lake書き込み時のアイソレーションレベルを「スナップショット分離」に設定\n",
    "            # 書き込み時にデータシャッフルを行い、大きなファイルを生成する機能の有効化\n",
    "            # 書き込み後に小さなファイルを自動で統合する機能の有効化\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b343d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env ファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# 環境変数の取得\n",
    "AI_FOUNDRY_ENDPOINT = os.environ.get(\"AI_FOUNDRY_ENDPOINT\")\n",
    "AI_FOUNDRY_API_KEY  = os.environ.get(\"AI_FOUNDRY_API_KEY\")\n",
    "AI_FOUNDRY_MODEL    = os.environ.get(\"AI_FOUNDRY_MODEL\")\n",
    "MAX_TOKENS          = os.environ.get(\"MAX_TOKENS\")\n",
    "TEMPERATURE         = os.environ.get(\"TEMPERATURE\")\n",
    "TOP_P               = os.environ.get(\"TOP_P\")\n",
    "\n",
    "# メモ：\n",
    "# ウィジット経由でのパラメータの取得方法では、無条件にパラメータの型がStringに変換されてしまう\n",
    "# そのため、受け取ったパラメータを適切に型変換する必要がある\n",
    "MAX_TOKENS  = int(MAX_TOKENS)\n",
    "TEMPERATURE = float(TEMPERATURE)\n",
    "TOP_P       = float(TOP_P)\n",
    "\n",
    "\n",
    "# 簡易デバッグ用\n",
    "print(f'AI_FOUNDRY_ENDPOINT: {AI_FOUNDRY_ENDPOINT}')\n",
    "print(f'AI_FOUNDRY_API_KEY:  {AI_FOUNDRY_API_KEY}')\n",
    "print(f'AI_FOUNDRY_MODEL:    {AI_FOUNDRY_MODEL}')\n",
    "print(f'MAX_TOKENS:          {MAX_TOKENS}')\n",
    "print(f'TEMPERATURE:         {TEMPERATURE}')\n",
    "print(f'TOP_P:               {TOP_P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger    = getLogger(__name__)\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "LP_URL   = \"https://lp.br-lb.com/\"\n",
    "\n",
    "res_dict = await fetch_web(logger, semaphore, LP_URL)\n",
    "print(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da514f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages  = []\n",
    "messages.append(SystemMessage(content=(\n",
    "\t\t\t\t\"あなたは商品LPの分析を行うマーケティングの専門家です。\\n\"\n",
    "\t\t\t\t\"提供された商品情報を読み解き、この商品が「マッチする領域（適合）」と「マッチしない領域（不適合）」を明確に分類して抽出してください。\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【出力形式（厳守）】\\n\"\n",
    "\t\t\t\t\"回答は必ず以下のJSON形式のみを出力してください。\\n\"\n",
    "\t\t\t\t\"各単語をキーとし、その関連度の強さ（重み）を 0.0〜1.0 の数値で値として設定してください。\\n\"\n",
    "\t\t\t\t\"Markdown記法（```json 等）は含めず、生のJSONテキストのみを返してください。\\n\\n\"\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"positive\": {\n",
    "\t\t\t\t\t\t\"places\": {\"単語\": 0.9, \"単語\": 0.7},\n",
    "\t\t\t\t\t\t\"scenes\": {\"単語\": 0.8, \"単語\": 0.6},\n",
    "\t\t\t\t\t\t\"traits\": {\"単語\": 0.9}\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"negative\": {\n",
    "\t\t\t\t\t\t\"places\": {\"単語\": 0.9},\n",
    "\t\t\t\t\t\t\"scenes\": {\"単語\": 0.8},\n",
    "\t\t\t\t\t\t\"traits\": {\"単語\": 0.7}\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t\"\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【分析項目と定義】\\n\"\n",
    "\t\t\t\t\"--- positive（適合）：商品が積極的に活用される文脈 ---\\n\"\n",
    "\t\t\t\t\"・places（場所）：実際に使われる場所（※可能な限り指定マスターリストから選択）\\n\"\n",
    "\t\t\t\t\"・scenes（場面）：利用状況やタイミング\\n\"\n",
    "\t\t\t\t\"・traits（行動・心理）：ターゲットユーザーの特徴\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"--- negative（不適合）：利用が想定されない、または不向きな文脈 ---\\n\"\n",
    "\t\t\t\t\"・places（場所）：適さない場所、不要な場所（※リスト外の言葉も可）\\n\"\n",
    "\t\t\t\t\"・scenes（場面）：機能が発揮できない状況\\n\"\n",
    "\t\t\t\t\"・traits（行動・心理）：この商品を必要としない人の特徴\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【重み（スコア）の基準】\\n\"\n",
    "\t\t\t\t\"・1.0に近いほど：その傾向が非常に強い、確信度が高い\\n\"\n",
    "\t\t\t\t\"・0.0に近いほど：関連性が薄い\\n\"\n",
    "\t\t\t\t\"・positiveの場合：適合度の高さ\\n\"\n",
    "\t\t\t\t\"・negativeの場合：不適合度の高さ（明確に避けるべき度合い）\"\n",
    "\t\t\t)))\n",
    "messages.append(UserMessage(content=json.dumps(res_dict)))\n",
    "\n",
    "llmClient = OpenAI(base_url=AI_FOUNDRY_ENDPOINT, api_key=AI_FOUNDRY_API_KEY)\n",
    "response  = llmClient.chat.completions.create(\n",
    "\t\t\t\tmessages=messages,\n",
    "\t\t\t\ttools=None,\n",
    "\t\t\t\ttool_choice=None,\n",
    "\t\t\t\tmax_tokens=MAX_TOKENS,\n",
    "\t\t\t\ttemperature=TEMPERATURE,\n",
    "\t\t\t\ttop_p=TOP_P,\n",
    "\t\t\t\tmodel=AI_FOUNDRY_MODEL\n",
    "\t\t\t)\n",
    "\n",
    "result_text = response.choices[0].message.content\n",
    "analysis_data = json.loads(result_text)\n",
    "analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_UPDATE_DATE   = datetime.now(tz=ZoneInfo('Asia/Tokyo')).replace(day=1) - timedelta(days=1)\n",
    "SPECIFIED_START_DATE = LATEST_UPDATE_DATE.replace(day=1).strftime('%Y-%m-%d')\n",
    "SPECIFIED_END_DATE   = LATEST_UPDATE_DATE.strftime('%Y-%m-%d')\n",
    "\n",
    "VISIT_BEHAVIOR_TABLE = \"adinte_datainfrastructure.master.relational_spot\"\n",
    "window_moving        = Window.partitionBy('adid')\n",
    "sdf_visit_behav      = spark.read.table(VISIT_BEHAVIOR_TABLE)\\\n",
    "\t\t\t\t\t\t\t.select(['adid', 'prefecture', 'city', 'countofcontact', 'start_date', 'end_date'])\\\n",
    "                            .filter(col('start_date') == SPECIFIED_START_DATE)\\\n",
    "                            .filter(col('end_date')   == SPECIFIED_END_DATE)\\\n",
    "                            .withColumn('location', F.concat_ws('', F.col('prefecture'), F.col('city')))\\\n",
    "                            .select( ['location', 'adid', 'countofcontact'])\\\n",
    "                            .groupBy(['location', 'adid'])\\\n",
    "                            .agg(F.sum('countofcontact').alias('countofcontact'))\\\n",
    "                            .withColumn('visit_frequency', col('countofcontact') / F.sum('countofcontact').over(window_moving))\\\n",
    "                            .filter(col('visit_frequency') != 1)\\\n",
    "                            .select( ['location', 'adid', 'visit_frequency'])\\\n",
    "                            .orderBy(['location', 'adid'])\n",
    "\n",
    "sdf_visit_behav = sdf_visit_behav.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "sdf_visit_behav.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "items            = list(analysis_data['positive']['places'].items())\n",
    "lp_keywords      = [k for k, v in items]\n",
    "lp_weights       = [v for k, v in items]\n",
    "relational_spots = [row['location'] for row in sdf_visit_behav.select('location').dropDuplicates().collect()]\n",
    "\n",
    "model            = SentenceTransformer('pkshatech/GLuCoSE-base-ja')\n",
    "lp_vector        = np.array(lp_weights).reshape(1, -1)               # 1 × キーワード数M\n",
    "lp_matrix        = model.encode(lp_keywords)                         # キーワード数M × 768\n",
    "spots_matrix     = model.encode(relational_spots)                    # スポット数N × 768\n",
    "lp_coefficient   = lp_vector @ lp_matrix @ spots_matrix.T            # LPのスポット係数\n",
    "lp_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_data = list(zip(relational_spots, lp_coefficient.flatten().astype(float).tolist()))\n",
    "schema = types.StructType([\n",
    "    types.StructField('location', types.StringType(), True),\n",
    "    types.StructField('weight',   types.DoubleType(), True)\n",
    "])\n",
    "sdf_weights = spark.createDataFrame(weights_data, schema)\\\n",
    "\t\t\t\t.withColumns({\n",
    "                    'mean'   : F.mean('weight').over(Window.partitionBy()),\n",
    "                    'stddev' : F.stddev('weight').over(Window.partitionBy()),\n",
    "\t\t\t\t})\\\n",
    "\t\t\t\t.withColumn('weight', (col('weight') - col('mean')) / col('stddev'))\\\n",
    "                .select(['location', 'weight'])\n",
    "\n",
    "sdf_scored = sdf_visit_behav\\\n",
    "    \t\t\t.join(sdf_weights, on='location', how='inner')\\\n",
    "    \t\t\t.withColumn('weighted_score', F.col('visit_frequency') * F.col('weight'))\\\n",
    "                .groupBy('adid')\\\n",
    "    \t\t\t.agg(F.sum('weighted_score').alias('final_score'))\\\n",
    "                .select(['adid', 'final_score'])\\\n",
    "                .orderBy('final_score')\\\n",
    "                .limit(10000)\n",
    "\n",
    "sdf_scored.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASON_THRESHOLD = 0.5\n",
    "sdf_reason       = sdf_weights\\\n",
    "\t\t\t\t\t\t.filter(col('weight') >= REASON_THRESHOLD)\\\n",
    "                        .join(sdf_visit_behav, on='location', how='inner')\\\n",
    "                        .select(['adid', 'location'])\\\n",
    "                        .groupBy('adid')\\\n",
    "                        .agg(F.concat_ws(', ', F.collect_set('location')).alias('reason'))\\\n",
    "                        .select(['adid', 'reason'])\\\n",
    "                \t\t.orderBy('adid')\\\n",
    "                        .limit(10000)\n",
    "\n",
    "sdf_reason.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要なデータフレームのメモリ解放\n",
    "sdf_visit_behav.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
