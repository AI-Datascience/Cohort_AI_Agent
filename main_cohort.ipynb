{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install sentencepiece protobuf\n",
    "%pip install python-dotenv==1.2.1\n",
    "%pip install openai==2.16.0\n",
    "%pip install readability-lxml==0.8.4.1 w3lib==2.4.0 httpx==0.28.1 beautifulsoup4==4.14.3 azure-ai-inference==1.0.0b9\n",
    "%pip install sentence-transformers==5.2.2 tiktoken==0.12.0\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6af442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import numpy  as np\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv   import load_dotenv\n",
    "from logging  import Logger, getLogger\n",
    "from openai   import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import types, Window\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from sentence_transformers     import SentenceTransformer\n",
    "\n",
    "from url_scraper import fetch_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f88cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder\\\n",
    "            .config(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\\\n",
    "            .config(\"spark.sql.parquet.output.committer.class\", \"org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\")\\\n",
    "            .config(\"spark.mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\\\n",
    "            .config(\"spark.sql.adaptive.enabled\", True)\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"auto\")\\\n",
    "            .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"100MB\")\\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\\\n",
    "            .config(\"spark.sql.dynamicPartitionPruning.enabled\", True)\\\n",
    "            .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\\\n",
    "            .config(\"spark.sql.session.timeZone\", \"Asia/Tokyo\")\\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "            .config(\"spark.databricks.delta.write.isolationLevel\", \"SnapshotIsolation\")\\\n",
    "            .config(\"spark.databricks.delta.optimizeWrite.enabled\", True)\\\n",
    "            .config(\"spark.databricks.delta.autoCompact.enabled\", True)\n",
    "            # Delta Lake 用の SQL コミットプロトコルを指定\n",
    "            # Parquet 出力時のコミッタークラスを指定\n",
    "            # Azure Blob Storage (ABFS) 用のコミッターファクトリを指定\n",
    "            # '_SUCCESS'で始まるファイルを書き込まないように設定\n",
    "            # AQE(Adaptive Query Execution)の有効化\n",
    "            # パーティション数を自動で調整するように設定\n",
    "            # シャッフル後の1パーティションあたりの最小サイズを指定\n",
    "            # AQEのパーティション合成の有効化\n",
    "            # 動的パーティションプルーニングの有効化\n",
    "            # 小さいテーブルのブロードキャスト結合への自動変換をするための閾値調整\n",
    "            # SparkSessionのタイムゾーンを日本標準時刻に設定\n",
    "            # Delta Lake固有のSQL構文や解析機能を拡張モジュールとして有効化\n",
    "            # SparkカタログをDeltaLakeカタログへ変更\n",
    "            # Delta Lake書き込み時のアイソレーションレベルを「スナップショット分離」に設定\n",
    "            # 書き込み時にデータシャッフルを行い、大きなファイルを生成する機能の有効化\n",
    "            # 書き込み後に小さなファイルを自動で統合する機能の有効化\n",
    "\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b343d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env ファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "# 環境変数の取得\n",
    "AI_FOUNDRY_ENDPOINT = os.environ.get(\"AI_FOUNDRY_ENDPOINT\")\n",
    "AI_FOUNDRY_API_KEY  = os.environ.get(\"AI_FOUNDRY_API_KEY\")\n",
    "AI_FOUNDRY_MODEL    = os.environ.get(\"AI_FOUNDRY_MODEL\")\n",
    "MAX_TOKENS          = os.environ.get(\"MAX_TOKENS\")\n",
    "TEMPERATURE         = os.environ.get(\"TEMPERATURE\")\n",
    "TOP_P               = os.environ.get(\"TOP_P\")\n",
    "\n",
    "# メモ：\n",
    "# ウィジット経由でのパラメータの取得方法では、無条件にパラメータの型がStringに変換されてしまう\n",
    "# そのため、受け取ったパラメータを適切に型変換する必要がある\n",
    "MAX_TOKENS  = int(MAX_TOKENS)\n",
    "TEMPERATURE = float(TEMPERATURE)\n",
    "TOP_P       = float(TOP_P)\n",
    "\n",
    "\n",
    "# 簡易デバッグ用\n",
    "print(f'AI_FOUNDRY_ENDPOINT: {AI_FOUNDRY_ENDPOINT}')\n",
    "print(f'AI_FOUNDRY_API_KEY:  {AI_FOUNDRY_API_KEY}')\n",
    "print(f'AI_FOUNDRY_MODEL:    {AI_FOUNDRY_MODEL}')\n",
    "print(f'MAX_TOKENS:          {MAX_TOKENS}')\n",
    "print(f'TEMPERATURE:         {TEMPERATURE}')\n",
    "print(f'TOP_P:               {TOP_P}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger    = getLogger(__name__)\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "LP_URL   = \"https://lp.br-lb.com/\"\n",
    "\n",
    "res_dict = await fetch_web(logger, semaphore, LP_URL)\n",
    "print(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da514f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages  = []\n",
    "messages.append(SystemMessage(content=(\n",
    "\t\t\t\t\"あなたは商品LPの分析を行うマーケティングの専門家です。\\n\"\n",
    "\t\t\t\t\"提供された商品情報を分析し、その商品が「最高に輝く具体的なシーン（適合）」と「全く役に立たない、あるいはミスマッチなシーン（不適合）」を洗い出してください。\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【重要な指示】\\n\"\n",
    "    \t\t\t\"抽出する単語は、単なる一般名詞ではなく、「誰が・どこで・何をしているか」がありありと想像できるような、具体的かつ解像度の高いキーワードを選定してください。\\n\"\n",
    "    \t\t\t\"特に「場所」に関しては、大分類（例：公園）ではなく、詳細な施設タイプ（例：ドッグラン、親水広場）や、利用目的が明確なスポット名を優先してください。\\n\\n\"\n",
    "\n",
    "\t\t\t\t\"【出力形式（厳守）】\\n\"\n",
    "\t\t\t\t\"回答は必ず以下のJSON形式のみを出力してください。\\n\"\n",
    "\t\t\t\t\"各単語をキーとし、その関連度の強さ（重み）を 0.0〜1.0 の数値で値として設定してください。\\n\"\n",
    "\t\t\t\t\"Markdown記法（```json 等）は含めず、生のJSONテキストのみを返してください。\\n\\n\"\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"positive\": {\"キーワードA\": 0.9, \"キーワードB\": 0.7, ...},\n",
    "\t\t\t\t\t\"negative\": {\"キーワードC\": 0.9, ...},\n",
    "\t\t\t\t}\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\t\"\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【分析の視点】\\n\"\n",
    "\t\t\t\t\"--- positive（適合）：商品が必須となる、または魅力を最大化する文脈 ---\\n\"\n",
    "\t\t\t\t\"1. 具体的な施設・スポット（Places）：\\n\"\n",
    "\t\t\t\t\"   - 抽象的な「店」「屋外」はNG。\\n\"\n",
    "\t\t\t\t\"   - 「24時間ジム」「オートキャンプ場」「コワーキングスペース」など、行動が特定できる施設名。\\n\"\n",
    "\t\t\t\t\"2. 利用シーン・瞬間（Scenes）：\\n\"\n",
    "\t\t\t\t\"   - 「通勤ラッシュ」「運動後のシャワー」「子供の寝かしつけ」など、具体的なタイムラインや状況。\\n\"\n",
    "\t\t\t\t\"3. ターゲットの属性・状態（Traits）：\\n\"\n",
    "\t\t\t\t\"   - 「健康志向」のような広い言葉より、「糖質制限中」「リモートワーク疲れ」など具体的な状態。\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"--- negative（不適合）：商品の機能が死ぬ、または邪魔になる文脈 ---\\n\"\n",
    "\t\t\t\t\"1. 阻害要因となる場所（Places）：\\n\"\n",
    "\t\t\t\t\"   - 商品のスペック（大きさ、音、電源有無など）的に使えない場所（例：図書館、満員電車）。\\n\"\n",
    "\t\t\t\t\"2. 無意味なシーン（Scenes）：\\n\"\n",
    "\t\t\t\t\"   - その商品をあえて使う必要がない状況。\\n\\n\"\n",
    "\t\t\t\t\n",
    "\t\t\t\t\"【重み（スコア）の基準】\\n\"\n",
    "\t\t\t\t\"・1.0に近いほど：その傾向が非常に強い、確信度が高い\\n\"\n",
    "\t\t\t\t\"・0.0に近いほど：関連性が薄い\\n\"\n",
    "\t\t\t\t\"・positiveの場合：適合度の高さ\\n\"\n",
    "\t\t\t\t\"・negativeの場合：不適合度の高さ（明確に避けるべき度合い）\"\n",
    "\t\t\t)))\n",
    "messages.append(UserMessage(content=json.dumps(res_dict)))\n",
    "\n",
    "llmClient = OpenAI(base_url=AI_FOUNDRY_ENDPOINT, api_key=AI_FOUNDRY_API_KEY)\n",
    "response  = llmClient.chat.completions.create(\n",
    "\t\t\t\tmessages=messages,\n",
    "\t\t\t\ttools=None,\n",
    "\t\t\t\ttool_choice=None,\n",
    "\t\t\t\tmax_tokens=MAX_TOKENS,\n",
    "\t\t\t\ttemperature=TEMPERATURE,\n",
    "\t\t\t\ttop_p=TOP_P,\n",
    "\t\t\t\tmodel=AI_FOUNDRY_MODEL\n",
    "\t\t\t)\n",
    "\n",
    "result_text = response.choices[0].message.content\n",
    "analysis_data = json.loads(result_text)\n",
    "analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEST_UPDATE_DATE   = datetime.now(tz=ZoneInfo('Asia/Tokyo')).replace(day=1) - timedelta(days=1)\n",
    "SPECIFIED_START_DATE = LATEST_UPDATE_DATE.replace(day=1).strftime('%Y-%m-%d')\n",
    "SPECIFIED_END_DATE   = LATEST_UPDATE_DATE.strftime('%Y-%m-%d')\n",
    "\n",
    "VISIT_BEHAVIOR_TABLE = \"adinte_datainfrastructure.master.relational_spot\"\n",
    "window_moving        = Window.partitionBy('adid')\n",
    "sdf_visit_behav      = spark.read.table(VISIT_BEHAVIOR_TABLE)\\\n",
    "\t\t\t\t\t\t\t.select(['adid', 'prefecture', 'city', 'countofcontact', 'start_date', 'end_date'])\\\n",
    "                            .filter(col('start_date') == SPECIFIED_START_DATE)\\\n",
    "                            .filter(col('end_date')   == SPECIFIED_END_DATE)\\\n",
    "                            .withColumn('location', F.concat_ws('', F.col('prefecture'), F.col('city')))\\\n",
    "                            .select( ['location', 'adid', 'countofcontact'])\\\n",
    "                            .groupBy(['location', 'adid'])\\\n",
    "                            .agg(F.sum('countofcontact').alias('countofcontact'))\\\n",
    "                            .withColumn('visit_frequency', col('countofcontact') / F.sum('countofcontact').over(window_moving))\\\n",
    "                            .filter(col('visit_frequency') != 1)\\\n",
    "                            .select( ['location', 'adid', 'visit_frequency'])\\\n",
    "                            .orderBy(['location', 'adid'])\n",
    "\n",
    "# sdf_visit_behav = sdf_visit_behav.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "sdf_visit_behav.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_positive   = list(analysis_data['positive'].items())\n",
    "items_negative   = list(analysis_data['negative'].items())\n",
    "lp_keywords      = [key for key, val in items_positive] + [ key for key, val in items_negative]\n",
    "lp_weights       = [val for key, val in items_positive] + [-val for key, val in items_negative]\n",
    "relational_spots = [row['location'] for row in sdf_visit_behav.select('location').dropDuplicates().collect()]\n",
    "\n",
    "model            = SentenceTransformer('pkshatech/GLuCoSE-base-ja')\n",
    "lp_vector        = np.array(lp_weights).reshape(1, -1)               # 1 × キーワード数M\n",
    "lp_matrix        = model.encode(lp_keywords)                         # キーワード数M × 768\n",
    "spots_matrix     = model.encode(relational_spots)                    # スポット数N × 768\n",
    "lp_coefficient   = lp_vector @ lp_matrix @ spots_matrix.T            # LPのスポット係数\n",
    "lp_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_data = list(zip(relational_spots, lp_coefficient.flatten().astype(float).tolist()))\n",
    "schema = types.StructType([\n",
    "    types.StructField('location', types.StringType(), True),\n",
    "    types.StructField('weight',   types.DoubleType(), True)\n",
    "])\n",
    "sdf_weights = spark.createDataFrame(weights_data, schema)\\\n",
    "\t\t\t\t.withColumn('weight', F.tanh('weight'))\\\n",
    "                .select(['location', 'weight'])\n",
    "\n",
    "sdf_scored = sdf_visit_behav\\\n",
    "    \t\t\t.join(sdf_weights, on='location', how='inner')\\\n",
    "    \t\t\t.withColumn('weighted_score', F.col('visit_frequency') * F.col('weight'))\\\n",
    "                .groupBy('adid')\\\n",
    "    \t\t\t.agg(F.sum('weighted_score').alias('final_score'))\\\n",
    "                .select(['adid', 'final_score'])\\\n",
    "                .orderBy(col('final_score').desc())\\\n",
    "                .limit(10000)\n",
    "\n",
    "sdf_scored.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REASON_THRESHOLD = 0.5\n",
    "sdf_reason       = sdf_weights\\\n",
    "\t\t\t\t\t\t.filter(col('weight') >= REASON_THRESHOLD)\\\n",
    "                        .join(sdf_visit_behav, on='location', how='inner')\\\n",
    "                        .select(['adid', 'location'])\\\n",
    "                        .groupBy('adid')\\\n",
    "                        .agg(F.concat_ws(', ', F.collect_set('location')).alias('reason'))\\\n",
    "                        .select(['adid', 'reason'])\\\n",
    "\t\t\t\t\t\t.join(sdf_scored.select('adid'), on='adid', how='inner')\\\n",
    "                \t\t.orderBy('adid')\n",
    "\n",
    "sdf_reason.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要なデータフレームのメモリ解放\n",
    "sdf_visit_behav.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
